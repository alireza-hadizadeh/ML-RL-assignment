
##  گزارش پروژه: پیاده‌سازی دروازه OR با استفاده از پرسپترون

###  مقدمه

شبکه‌های عصبی مصنوعی (Artificial Neural Networks) با الهام از ساختار مغز انسان طراحی شده‌اند. یکی از ساده‌ترین مدل‌های این شبکه‌ها، **پرسپترون** (Perceptron) است که توسط Frank Rosenblatt معرفی شد و قادر به یادگیری مسائل **خطی قابل تفکیک** مانند دروازه‌های منطقی ساده (OR، AND) می‌باشد.

در این پروژه، قصد داریم **دروازه منطقی OR** را با استفاده از یک **پرسپترون تک لایه** (Single-Layer Perceptron) پیاده‌سازی کنیم.

---

###  داده‌های ورودی و خروجی هدف

دروازه OR در منطق دیجیتال به صورت زیر تعریف می‌شود:

| x1 | x2 | خروجی هدف t |
| -- | -- | ----------- |
| 0  | 0  | 0           |
| 0  | 1  | 1           |
| 1  | 0  | 1           |
| 1  | 1  | 1           |

---

###  معماری مدل

مدل ما شامل:

* دو ورودی: x1 و x2
* دو وزن متناظر: w1 و w2
* یک آستانه (Threshold یا θ که در کد برابر صفر در نظر گرفته شده)
* تابع فعال‌سازی: تابع آستانه (Step Function)

فرمول خروجی پرسپترون:

$$
y = f(w_1 x_1 + w_2 x_2 - \theta)
$$

که در آن تابع $f$ برابر است با:

$$
f(x) = \begin{cases}
1 & \text{اگر } x \geq 0 \\
0 & \text{در غیر این صورت}
\end{cases}
$$

---

###  الگوریتم آموزش: قانون دلتا (Delta Rule)

برای به‌روزرسانی وزن‌ها از **قانون دلتا** استفاده می‌کنیم:

$$
w_i \leftarrow w_i + \alpha \cdot e \cdot x_i
$$

که در آن:

* $\alpha$ نرخ یادگیری (Learning Rate) است
* $e = t - y$ مقدار خطا
* $x_i$ ورودی مرتبط با وزن

---

###  پیاده‌سازی کد (Python)

```python
def step(x):
    return 1 if x >= 0 else 0

X = [[0, 0], [0, 1], [1, 0], [1, 1]]
T = [0, 1, 1, 1]  # خروجی هدف OR

w1 = -0.2
w2 = 0.4
theta = 0
learning_rate = 0.2

for epoch in range(3):
    print(f"\nEpoch {epoch + 1}")
    for i in range(4):
        x1, x2 = X[i]
        t = T[i]
        net = x1 * w1 + x2 * w2 - theta
        y = step(net)
        e = t - y
        w1 = w1 + learning_rate * e * x1
        w2 = w2 + learning_rate * e * x2
        print(f"x1={x1} x2={x2} t={t} y={y} e={e} -> w1={w1:.2f}, w2={w2:.2f}")
```

---

###  خروجی نمونه از اجرای کد

```
Epoch 1
x1=0 x2=0 t=0 y=0 e=0 -> w1=-0.20, w2=0.40
x1=0 x2=1 t=1 y=1 e=0 -> w1=-0.20, w2=0.40
x1=1 x2=0 t=1 y=0 e=1 -> w1=0.00, w2=0.40
x1=1 x2=1 t=1 y=1 e=0 -> w1=0.00, w2=0.40
...
```

در پایان هر epoch، وزن‌ها به‌روز شده و مدل به تدریج به خروجی‌های هدف نزدیک‌تر می‌شود.

---

###  نتیجه‌گیری

این پیاده‌سازی ساده نشان می‌دهد که چگونه یک پرسپترون با استفاده از مفاهیم پایه مانند **تابع آستانه، قانون دلتا، و به‌روزرسانی وزن‌ها** می‌تواند عملکرد یک گیت منطقی OR را یاد بگیرد.

اگرچه پرسپترون تنها مسائل **خطی قابل تفکیک** را می‌تواند یاد بگیرد، اما پایه‌ای برای درک شبکه‌های چند لایه و الگوریتم backpropagation در مدل‌های پیچیده‌تر است.

