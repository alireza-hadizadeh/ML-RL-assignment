###  مقدمه:

در این پروژه، الگوریتم Q(λ) از صفر پیاده‌سازی شده است. این الگوریتم نسخه‌ای بهبودیافته از Q-learning است که از **ردگیری صلاحیت‌ها (Eligibility Traces)** برای سرعت‌بخشی به یادگیری استفاده می‌کند.

---

###  جزئیات محیط:

* محیط به صورت ماتریسی (GridWorld) طراحی شده است.
* شامل وضعیت شروع (S)، وضعیت هدف (G)، دیوار (#)، و فضاهای خالی (-) می‌باشد.
* عامل با چهار حرکت اصلی حرکت می‌کند: بالا، پایین، چپ و راست.
* برخورد با دیوار موجب ماندن در جای فعلی و دریافت پاداش منفی می‌شود.
* رسیدن به خانه G پایان اپیزود و پاداش مثبت +1 دارد.
* سایر حرکات هزینه‌ی -0.01 دارند.

---

###  پارامترهای یادگیری:

| پارامتر         | مقدار |
| --------------- | ----- |
| نرخ یادگیری (α) | 0.1   |
| نرخ تخفیف (γ)   | 0.99  |
| λ               | 0.8   |
| ε (اکتشاف)      | 0.1   |

---

###  الگوریتم Q(λ):

* جدول Q برای هر state-action نگهداری می‌شود.
* در هر گام:

  * Q\[state, action] به‌روز می‌شود بر اساس TD error (δ).
  * eligibility trace برای جفت state-action جاری افزایش می‌یابد.
  * برای همه‌ی state-actionها، Q با استفاده از trace به‌روز می‌شود.
  * trace با ضریب γλ کاهش می‌یابد.

---

###  ارزیابی سیاست نهایی:

بعد از آموزش Q، عملکرد سیاست با اجرای 100 اپیزود تست بررسی شد. عامل در این ارزیابی بدون اکتشاف (ε=0) عمل کرد.

```
 Success rate: xx.xx% in 100 test episodes.
```

اگر درصد موفقیت بیشتر از 90٪ باشد، نشان از یادگیری مؤثر دارد.

---

###  نتیجه‌گیری:

* الگوریتم Q(λ) سرعت یادگیری بالاتری نسبت به Q-learning ساده دارد.
* با محیط‌های ساده‌ای مانند GridWorld قابل آزمایش است.
* امکان گسترش به محیط‌های پیچیده‌تر یا پیاده‌سازی با گراف یا بازی نیز وجود دارد.
