# DGTA-RL: Dynamic Graph Temporal Attention Reinforcement Learning

This repository contains a PyTorch implementation of the DGTA-RL model, designed to solve the **Dynamic Traveling Salesman Problem with Time-Dependent and Stochastic Travel Times (DTSP-TDS)**.

## ðŸ“¦ Project Structure

```
DGTA-RL/
â”œâ”€â”€ model/              # Modular model components
â”‚   â”œâ”€â”€ dgta.py
â”‚   â”œâ”€â”€ dual_attention.py
â”‚   â”œâ”€â”€ dynamic_encoder.py
â”‚   â”œâ”€â”€ pointer_decoder.py
â”‚   â”œâ”€â”€ positional.py
â”œâ”€â”€ env.py              # DTSP-TDS environment simulator
â”œâ”€â”€ train.py            # REINFORCE training loop with logging
â”œâ”€â”€ eval.py             # Evaluation on test instances
â”œâ”€â”€ plot.py             # Visualizes reward trends
â”œâ”€â”€ utils.py            # Helper functions
â””â”€â”€ dgta_rl.log         # Training reward logs (generated after training)
```

## ðŸš€ Getting Started

### 1. Clone and Install

```bash
git clone https://github.com/alireza-hadizadeh/ML-RL-assignment.git
cd ML-RL-assignment/DGTA-RL
pip install -r requirements.txt  # (Coming soon)
```

### 2. Train the Model

```bash
python -m train
```

This trains the DGTA model on synthetic DTSP-TDS instances.

### 3. Evaluate the Trained Model

```bash
python eval.py
```

Outputs average, best, and worst tour quality over 100 test instances.

### 4. Plot Reward Progress

```bash
python plot.py
```

Saves a PNG showing training reward curves for policy vs baseline.

## ðŸ§  Model Highlights

* **Graph Transformer with Dual Attention (DGTA)**
* Learns from scratch using **REINFORCE**
* **Gamma-distributed** stochastic travel times
* **Paired t-test** policy update (baseline-aware)

## ðŸ“ Notes

* Trained models are saved as `dgta_rl.pt`
* Intermediate checkpoints saved as `dgta_rl_iter_XXXX.pt`
* Logging format: `dgta_rl.log`

## ðŸ“š Reference

Based on the 2025 article:

> Chen et al., "The Dynamic Traveling Salesman Problem with Time-Dependent and Stochastic Travel Times: A Deep Reinforcement Learning Approach"

---

For questions or improvements, feel free to open issues or pull requests! ðŸ’¡
